{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pcx4M2b6FbY"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTKqga__YonT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title External Imports\n",
        "import torch\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "901eUnioSIO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47d4cb8-c12e-4dd0-d09c-449d95932f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.nn.conv import SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.loader import DataLoader as ShannonDataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "from torch_scatter import scatter, scatter_softmax\n",
        "\n",
        "from math import sqrt, ceil\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lxGLnXNoKMM"
      },
      "outputs": [],
      "source": [
        "#@title Hyperparameters and Constants\n",
        "board_width = 11\n",
        "num_tiles = board_width**2\n",
        "\n",
        "# Define the hyperparameters we are going to use:\n",
        "gnn_params = {\n",
        "    'input_dim': 3,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 9,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 1e-4,\n",
        "    'drop_ratio': 0\n",
        "}\n",
        "\n",
        "gin_params = {\n",
        "    'input_dim': 3,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 9,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 1e-4,\n",
        "    'drop_ratio': 0\n",
        "}\n",
        "\n",
        "cnn_params = {\n",
        "    'input_dim': 3,\n",
        "    'hidden_dim': 128,\n",
        "    'output_dim': num_tiles+1,\n",
        "    'num_layers': 10,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 1e-4,\n",
        "    'drop_ratio': 0\n",
        "}\n",
        "\n",
        "hex_grid_params = {\n",
        "    'input_dim': 3,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 10,\n",
        "    'learning_rate': 1e-4,#5e-5,\n",
        "    'weight_decay': 1e-4,\n",
        "    'drop_ratio': 0\n",
        "}\n",
        "\n",
        "det_shannon_params = {\n",
        "    'input_dim': 2,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 10,\n",
        "    'learning_rate': 1e-4,#5e-5,\n",
        "    'weight_decay': 1e-4,\n",
        "    'drop_ratio': 0\n",
        "}\n",
        "\n",
        "rni_shannon_params = {\n",
        "    'input_dim': 2,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 10,\n",
        "    'random_dims': 6,\n",
        "    'learning_rate': 1e-4,#5e-5,\n",
        "    'weight_decay': 1e-4,\n",
        "    'drop_ratio': 0\n",
        "}\n",
        "\n",
        "sage_params = {\n",
        "    'input_dim': 2,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 10,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 1e-4,\n",
        "    'drop_ratio': 0\n",
        "}\n",
        "\n",
        "params = {\n",
        "    'num_epochs': 100,\n",
        "    'save_interval': 2, # 0 to disable saves entirely\n",
        "    'plot_interval': 1, # 0 to disable plots entirely\n",
        "    'early_stop_threshold': 10,\n",
        "    'policy_loss_fn': nn.CrossEntropyLoss(reduction='mean'),\n",
        "    'value_loss_fn': nn.MSELoss(),\n",
        "    'loss_weight': 0.5,\n",
        "\n",
        "    'generate_and_save_triples': False,\n",
        "    'generate_and_save_shannon_triples': False,\n",
        "    'plot_initial_outputs': False,\n",
        "\n",
        "    'load_gnn': False,\n",
        "    'load_cnn': False,\n",
        "\n",
        "    'load_hex_grid':    False,\n",
        "    'load_det_shannon': False,\n",
        "    'load_rni_shannon': False,\n",
        "    'load_cnn': False,\n",
        "    'load_sage': False,\n",
        "\n",
        "    'new_hex_grid': True,\n",
        "    'new_det_shannon': False,\n",
        "    'new_rni_shannon': False,\n",
        "    'new_cnn': True,\n",
        "    'new_sage': False,\n",
        "\n",
        "    'act_fun': F.relu,\n",
        "    'batch_size': 1024,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niNkSHGalK9_"
      },
      "outputs": [],
      "source": [
        "#@title Load Models\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model_checkpoint_filenames_to_load = []\n",
        "\n",
        "if params['load_hex_grid']:\n",
        "    action_acc  = \"0.37\"\n",
        "    outcome_acc = \"0.8\"\n",
        "    tr_epoch    = \"75\"\n",
        "    network_name = \"HexGridGNN\"\n",
        "    model_checkpoint_filenames_to_load.append(\n",
        "        f'/content/drive/MyDrive/Colab Notebooks/hexbot/temp/action-{action_acc}_E{tr_epoch}_{network_name}_D10_outcome{outcome_acc}.pt'\n",
        "    )\n",
        "\n",
        "if params['load_det_shannon']:\n",
        "    action_acc  = \"0.42\"\n",
        "    outcome_acc = \"0.76\"\n",
        "    tr_epoch    = \"75\"\n",
        "    network_name = \"ShannonGNN\"\n",
        "    model_checkpoint_filenames_to_load.append(\n",
        "        f'/content/drive/MyDrive/Colab Notebooks/hexbot/temp/action-{action_acc}_E{tr_epoch}_{network_name}_D10_outcome{outcome_acc}.pt'\n",
        "    )\n",
        "\n",
        "if params['load_rni_shannon']:\n",
        "    action_acc  = \"0.42\"\n",
        "    outcome_acc = \"0.74\"\n",
        "    tr_epoch    = \"75\"\n",
        "    network_name = \"ShannonGNN-RNI\"\n",
        "    model_checkpoint_filenames_to_load.append(\n",
        "        f'/content/drive/MyDrive/Colab Notebooks/hexbot/temp/action-{action_acc}_E{tr_epoch}_{network_name}_D10_outcome{outcome_acc}.pt'\n",
        "    )\n",
        "\n",
        "if params['load_cnn']:\n",
        "    action_acc  = \"0.28\"\n",
        "    outcome_acc = \"0.84\"\n",
        "    tr_epoch    = \"76\"\n",
        "    network_name = \"CNN\"\n",
        "    model_checkpoint_filenames_to_load.append(\n",
        "        f'/content/drive/MyDrive/Colab Notebooks/hexbot/temp/action-{action_acc}_E{tr_epoch}_{network_name}_D10_outcome{outcome_acc}.pt'\n",
        "    )\n",
        "\n",
        "if params['load_sage']:\n",
        "    action_acc  = \"0.32\"\n",
        "    outcome_acc = \"0.6\"\n",
        "    tr_epoch    = \"75\"\n",
        "    network_name = \"GraphSAGE\"\n",
        "    model_checkpoint_filenames_to_load.append(\n",
        "        f'/content/drive/MyDrive/Colab Notebooks/hexbot/temp/action-{action_acc}_E{tr_epoch}_{network_name}_D10_outcome{outcome_acc}.pt'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqpO5nsqUFfl"
      },
      "outputs": [],
      "source": [
        "def is_coord_in_board(coord):\n",
        "    x, y = coord\n",
        "    if x < board_width and x >= 0:\n",
        "        if y < board_width and y >= 0:\n",
        "            return True\n",
        "\n",
        "def index_from_position(row, col):\n",
        "    return col + row * board_width\n",
        "\n",
        "def generate_hex_grid_adj_matrix():\n",
        "    adj_matrix = torch.zeros(num_tiles, num_tiles)\n",
        "\n",
        "    for i in range(board_width):\n",
        "        for j in range(board_width):\n",
        "            potential_neighbours = [(i, j+1), (i+1, j), (i-1, j), (i, j-1), (i-1, j+1), (i+1, j-1)]\n",
        "            for (x,y) in potential_neighbours:\n",
        "                if is_coord_in_board((x,y)):\n",
        "                    adj_matrix[index_from_position(i,j)][index_from_position(x,y)] = 1\n",
        "    return adj_matrix\n",
        "\n",
        "\n",
        "from queue import Queue\n",
        "\n",
        "def generate_3d_node_coords():\n",
        "    centre_3d_coord = (0,0,0)\n",
        "    centre_2d_coord = [(board_width-1)//2, (board_width-1)//2]\n",
        "\n",
        "    queue = Queue()\n",
        "    queue.put((centre_3d_coord, centre_2d_coord))\n",
        "\n",
        "    all_3d_node_coords = torch.zeros(num_tiles, 3)\n",
        "\n",
        "    seen_2d_coords = set()\n",
        "\n",
        "    while not queue.empty():\n",
        "        current_3d_position, current_2d_position = queue.get()\n",
        "        all_3d_node_coords[index_from_position(*current_2d_position)] = torch.tensor(current_3d_position)\n",
        "\n",
        "        offsets_3d = [(0,1,-1), (0,-1,1), (1,0,-1), (-1,0,1), (1,-1,0), (-1,1,0)]\n",
        "        corresponding_2d_offsets = [(0,1), (0,-1), (1,0), (-1,0), (1,1), (-1,-1)]\n",
        "        for i, offset_3d in enumerate(offsets_3d):\n",
        "            offset_2d = corresponding_2d_offsets[i]\n",
        "            new_2d_coord = tuple(map(sum, zip(offset_2d, current_2d_position)))\n",
        "            new_3d_coord = tuple(map(sum, zip(offset_3d, current_3d_position)))\n",
        "            if is_coord_in_board(new_2d_coord) and new_2d_coord not in seen_2d_coords:\n",
        "                queue.put([new_3d_coord, new_2d_coord])\n",
        "                seen_2d_coords.add(new_2d_coord)\n",
        "\n",
        "    return all_3d_node_coords\n",
        "\n",
        "def display_matrix(matrix):\n",
        "    for i in range(len(matrix)):\n",
        "        for j in range(len(matrix)):\n",
        "            print(str(int(matrix[i][j].item())), end=' ')\n",
        "        print()\n",
        "\n",
        "adj_matrix = generate_hex_grid_adj_matrix().to(device)\n",
        "all_3d_node_coords = generate_3d_node_coords().to(device)\n",
        "\n",
        "global_edge_index = adj_matrix.nonzero().t().contiguous()\n",
        "#adj_matrix = adj_matrix.to_sparse()\n",
        "\n",
        "tile_relevances = torch.zeros(num_tiles, 3)\n",
        "for i, pos in enumerate(all_3d_node_coords):\n",
        "    x, y, z = pos\n",
        "    tile_relevances[i] = torch.tensor([torch.abs(x), torch.abs(y), (torch.sign(x)+torch.sign(y))/2 + 1])\n",
        "\n",
        "positions = torch.zeros(520, num_tiles, 3).to(device)\n",
        "#print(tile_relevances)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvUnvODC5V7C"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mO8Hz4RDF5k"
      },
      "outputs": [],
      "source": [
        "#@title Board Object\n",
        "# We need the data input to our model to be a tensor of shape (num_tiles x input_embedding_dim).\n",
        "# We want out data labels to be an index in range(num_tiles)\n",
        "EMPTY_VECTOR = torch.tensor([1.,0.,0.])\n",
        "WHITE_VECTOR = torch.tensor([0.,1.,0.])\n",
        "BLACK_VECTOR = torch.tensor([0.,0.,1.])\n",
        "\n",
        "class Board:\n",
        "    WHITE = 'W'\n",
        "    BLACK = 'B'\n",
        "\n",
        "    def __init__(self, width=board_width):\n",
        "        self.state = torch.tile(EMPTY_VECTOR, (num_tiles, 1))\n",
        "    def get_index_from_position(self, position):\n",
        "        col = ord(position[0])-ord('a')\n",
        "        row = int(position[1:])-1\n",
        "        return row*board_width + col\n",
        "\n",
        "    def update_state(self, position, colour):\n",
        "        index = self.get_index_from_position(position)\n",
        "        tile_vector = WHITE_VECTOR if colour == self.WHITE else BLACK_VECTOR\n",
        "        assert torch.equal(self.state[index], EMPTY_VECTOR)\n",
        "        #self.state[index] = WHITE_VECTOR if colour == self.WHITE else BLACK_VECTOR\n",
        "        self.state[index] = tile_vector\n",
        "        #assert self.state.shape == (num_tiles, len(EMPTY_VECTOR))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUE0LrVA63a5"
      },
      "outputs": [],
      "source": [
        "#@title Get State, Action, Outcome Triples\n",
        "def get_state_action_outcome_triples_from_file(game_data_filename):\n",
        "    states = []\n",
        "    actions = []\n",
        "    outcomes = []\n",
        "\n",
        "    with open(game_data_filename, 'r') as file:\n",
        "        intro_line = file.readline()\n",
        "        for line in tqdm(file.readlines()):\n",
        "            raw_game_string_array = line.split(' ')[:-1]\n",
        "            last_player = raw_game_string_array[-1][0]\n",
        "            outcome_wrt_last_player = int(float(line.split(' ')[-1]))\n",
        "            outcome_wrt_white_player = outcome_wrt_last_player if last_player == 'W' else -outcome_wrt_last_player\n",
        "            outcome_wrt_black_player = -outcome_wrt_white_player\n",
        "\n",
        "            normal_board = Board()\n",
        "            inverse_board = Board()\n",
        "            for i, raw_move_string in enumerate(raw_game_string_array):\n",
        "                first_open_bracket = raw_move_string.find('[')\n",
        "                first_close_bracket = raw_move_string.find(']')\n",
        "\n",
        "                white_turn = i%2 == 0\n",
        "                position = raw_move_string[first_open_bracket+1:first_close_bracket]\n",
        "\n",
        "                # We only create the state, action pairs for the second move onwards.\n",
        "                # This is because the first move is random, and not chosen by the bot.\n",
        "                if i > 0:\n",
        "                    second_open_bracket = raw_move_string.find('[', first_close_bracket+1)\n",
        "                    second_close_bracket = raw_move_string.find(']', first_close_bracket+1)\n",
        "\n",
        "                    position_probability_pairs = raw_move_string[second_open_bracket+1:second_close_bracket].split(';')[:-1]\n",
        "                    probability_vector = torch.zeros(num_tiles)\n",
        "\n",
        "                    for s in position_probability_pairs:\n",
        "                        action, probability = s.split(':')\n",
        "                        index = normal_board.get_index_from_position(action) #if white_turn else rotated_board.get_index_from_position(action)\n",
        "                        probability_vector[index] = float(probability)\n",
        "\n",
        "                    # Scale the non-zero elements so that they sum to 1.\n",
        "                    probability_vector /= torch.sum(probability_vector)\n",
        "\n",
        "                    actions.append(probability_vector)\n",
        "                    state_to_add = normal_board.state if white_turn else inverse_board.state\n",
        "                    states.append(torch.clone(state_to_add).detach())\n",
        "\n",
        "                    outcomes.append(outcome_wrt_white_player if white_turn else outcome_wrt_black_player)\n",
        "\n",
        "                # Update the board state, ready for the next move\n",
        "                normal_board.update_state(position, 'W' if white_turn else 'B')\n",
        "                inverse_board.update_state(position, 'B' if white_turn else 'W')\n",
        "\n",
        "    return states, actions, outcomes\n",
        "\n",
        "filename = 'drive/MyDrive/Colab Notebooks/hexbot/game_data_triples.pt'\n",
        "if params['generate_and_save_triples']:\n",
        "    states, actions, outcomes = get_state_action_outcome_triples_from_file('drive/MyDrive/Colab Notebooks/hexbot/game_data')\n",
        "    torch.save([states, actions, outcomes], filename)\n",
        "elif params['new_hex_grid'] or params['new_cnn'] or params['load_cnn'] or params['load_hex_grid']:\n",
        "    states, actions, outcomes = torch.load(filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Shannon Board Object\n",
        "def generate_node_adjacency_sets(inverse=False):\n",
        "    adjacency_sets_by_node = { n:set() for n in range(num_tiles+2) }\n",
        "\n",
        "    for i in range(board_width):\n",
        "        for j in range(board_width):\n",
        "            potential_neighbours = [(i, j+1), (i+1, j), (i-1, j), (i, j-1), (i-1, j+1), (i+1, j-1)]\n",
        "            for (x,y) in potential_neighbours:\n",
        "                if is_coord_in_board((x,y)):\n",
        "                    adjacency_sets_by_node[index_from_position(i,j)].add(index_from_position(x,y))\n",
        "    if not inverse:\n",
        "        adjacency_sets_by_node[num_tiles]   = {i*board_width   for i in range(board_width)}\n",
        "        adjacency_sets_by_node[num_tiles+1] = {i*board_width-1 for i in range(1, board_width+1)}\n",
        "        for i in range(board_width):\n",
        "            adjacency_sets_by_node[i*board_width].add(num_tiles)\n",
        "            adjacency_sets_by_node[((i+1)*board_width)-1].add(num_tiles+1)\n",
        "    else:\n",
        "        adjacency_sets_by_node[num_tiles]   = {i for i in range(board_width)}\n",
        "        adjacency_sets_by_node[num_tiles+1] = {num_tiles-i for i in range(1,board_width+1)}\n",
        "        for i in range(board_width):\n",
        "            adjacency_sets_by_node[i].add(num_tiles)\n",
        "            adjacency_sets_by_node[num_tiles-(i+1)].add(num_tiles+1)\n",
        "    return adjacency_sets_by_node\n",
        "\n",
        "PLAYABLE_VECTOR = torch.tensor([1.,0.]).unsqueeze(0)\n",
        "EDGE_VECTOR = torch.tensor([0.,1.]).unsqueeze(0)\n",
        "\n",
        "class ShannonEquivBoard:\n",
        "    def __init__(self, inverse=False):\n",
        "        self.state = generate_node_adjacency_sets(inverse)\n",
        "        self.node_name_lookup = { n:n for n in range(num_tiles+2) }\n",
        "        self.original_num_nodes = num_tiles + 2\n",
        "        self.num_playable_tiles = num_tiles\n",
        "\n",
        "    def get_index_from_position(self, position):\n",
        "        col = ord(position[0])-ord('a')\n",
        "        row = int(position[1:])-1\n",
        "        return row*board_width + col\n",
        "\n",
        "    def update_state(self, position, colour):\n",
        "        self.num_playable_tiles -= 1\n",
        "        index = self.get_index_from_position(position)\n",
        "        neighbours = self.state[index]\n",
        "        for node_to_make_smaller in range(index+1, self.original_num_nodes):\n",
        "            self.node_name_lookup[node_to_make_smaller] -= 1\n",
        "\n",
        "        for node in neighbours:\n",
        "            self.state[node].remove(index)\n",
        "            if colour == 'W':\n",
        "                self.state[node] = self.state[node] | neighbours\n",
        "                self.state[node].remove(node)\n",
        "        self.state[index] = set()\n",
        "\n",
        "    def get_graph(self, move_index, outcome):\n",
        "        top = []\n",
        "        bottom = []\n",
        "        for node, neighbours in self.state.items():\n",
        "            for neighbour in neighbours:\n",
        "                top.append(self.node_name_lookup[node])\n",
        "                bottom.append(self.node_name_lookup[neighbour])\n",
        "        edge_index = torch.tensor([top,bottom]).long()\n",
        "        x = torch.cat([torch.tile(PLAYABLE_VECTOR, (self.num_playable_tiles,1)), EDGE_VECTOR, EDGE_VECTOR], dim=0)\n",
        "        return Data(x=x, edge_index=edge_index, y=move_index, outcome=outcome)\n"
      ],
      "metadata": {
        "id": "ySF-6L1oJC32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get Shannon Graph, Action, Outcome Triples\n",
        "def get_shannon_graphs_from_file(game_data_filename):\n",
        "    graphs = []\n",
        "\n",
        "    with open(game_data_filename, 'r') as file:\n",
        "        intro_line = file.readline()\n",
        "        for line in tqdm(file.readlines()):\n",
        "            raw_game_string_array = line.split(' ')[:-1]\n",
        "            last_player = raw_game_string_array[-1][0]\n",
        "            outcome_wrt_last_player = int(float(line.split(' ')[-1]))\n",
        "            outcome_wrt_white_player = outcome_wrt_last_player if last_player == 'W' else -outcome_wrt_last_player\n",
        "            outcome_wrt_black_player = -outcome_wrt_white_player\n",
        "\n",
        "            normal_board = ShannonEquivBoard()\n",
        "            inverse_board = ShannonEquivBoard(inverse=True)\n",
        "            for i, raw_move_string in enumerate(raw_game_string_array):\n",
        "                first_open_bracket = raw_move_string.find('[')\n",
        "                first_close_bracket = raw_move_string.find(']')\n",
        "\n",
        "                white_turn = i%2 == 0\n",
        "                position = raw_move_string[first_open_bracket+1:first_close_bracket]\n",
        "\n",
        "                # We only create the state, action pairs for the second move onwards.\n",
        "                # This is because the first move is random, and not chosen by the bot.\n",
        "                if i > 0:\n",
        "                    second_open_bracket = raw_move_string.find('[', first_close_bracket+1)\n",
        "                    second_close_bracket = raw_move_string.find(']', first_close_bracket+1)\n",
        "\n",
        "                    position_probability_pairs = raw_move_string[second_open_bracket+1:second_close_bracket].split(';')[:-1]\n",
        "                    probability_vector = torch.zeros(normal_board.num_playable_tiles+2 if white_turn else inverse_board.num_playable_tiles+2)\n",
        "\n",
        "                    for s in position_probability_pairs:\n",
        "                        action, probability = s.split(':')\n",
        "                        index = normal_board.get_index_from_position(action) #if white_turn else rotated_board.get_index_from_position(action)\n",
        "                        if white_turn and normal_board.node_name_lookup[index] != 'X':\n",
        "                            probability_vector[normal_board.node_name_lookup[index]] = float(probability)\n",
        "                        elif not white_turn and inverse_board.node_name_lookup[index] != 'X':\n",
        "                            probability_vector[inverse_board.node_name_lookup[index]] = float(probability)\n",
        "\n",
        "                    # We now have the probability vector, we want to remove all entries corresponding to\n",
        "                    # Scale the non-zero elements so that they sum to 1.\n",
        "                    move_index = torch.argmax(probability_vector).unsqueeze(0)\n",
        "\n",
        "                    graph_to_add = normal_board.get_graph(move_index, outcome_wrt_white_player) if white_turn else inverse_board.get_graph(move_index, outcome_wrt_black_player)\n",
        "                    graphs.append(graph_to_add.clone().detach())\n",
        "\n",
        "                # Update the board state, ready for the next move\n",
        "                normal_board.update_state(position, 'W' if white_turn else 'B')\n",
        "                inverse_board.update_state(position, 'B' if white_turn else 'W')\n",
        "\n",
        "    return graphs\n",
        "\n",
        "filename = f'drive/MyDrive/Colab Notebooks/hexbot/shannon_graphs.pt'\n",
        "if params['generate_and_save_shannon_triples']:\n",
        "    print('generating and saving')\n",
        "    shannon_graphs = get_shannon_graphs_from_file('drive/MyDrive/Colab Notebooks/hexbot/game_data')\n",
        "    torch.save(shannon_graphs, filename)\n",
        "elif params['new_det_shannon'] or params['new_rni_shannon'] or params['new_sage'] or params['load_det_shannon'] or params['load_rni_shannon'] or params['load_sage']:\n",
        "    shannon_graphs = torch.load(filename)\n",
        "\n"
      ],
      "metadata": {
        "id": "ARN5VtRdcPZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BbuJHRoBefJU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp-T9_8OjuoW"
      },
      "outputs": [],
      "source": [
        "#@title Visualise State Action Pairs\n",
        "def plot_hexagonal_grid_with_colors(size, color_keys, probability_mode=False, rotated=False):\n",
        "    colors = np.array([(0.75,0.75,0.75), (1,1,1), (0,0,0)])\n",
        "    left_and_right_color = colors[1] if not rotated else colors[2]\n",
        "    top_and_bottom_color = colors[2] if not rotated else colors[1]\n",
        "\n",
        "    _, ax = plt.subplots()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')\n",
        "\n",
        "    hex_size = 1\n",
        "    horizontal_spacing = np.sqrt(3) * hex_size\n",
        "    vertical_spacing = 3/2 * hex_size\n",
        "    for col in range(1, size-1):\n",
        "        x_top = col * horizontal_spacing + (size-1)*horizontal_spacing/2\n",
        "        y_bottom = (size-1) * vertical_spacing\n",
        "        x_bottom = col * horizontal_spacing\n",
        "        y_top = 0\n",
        "        hex_border_top_outline = patches.RegularPolygon((x_top, y_top), numVertices=6, radius=hex_size, orientation=np.radians(0), edgecolor='black', facecolor='none', lw=20, zorder=2)\n",
        "        hex_border_bottom_outline = patches.RegularPolygon((x_bottom, y_bottom), numVertices=6, radius=hex_size, orientation=np.radians(0), edgecolor='black', facecolor='none', lw=20, zorder=2)\n",
        "        hex_border_top = patches.RegularPolygon((x_top, y_top), numVertices=6, radius=hex_size, orientation=np.radians(0), edgecolor=top_and_bottom_color, facecolor='none', lw=18, zorder=3)\n",
        "        hex_border_bottom = patches.RegularPolygon((x_bottom, y_bottom), numVertices=6, radius=hex_size, orientation=np.radians(0), edgecolor=top_and_bottom_color, facecolor='none', lw=18, zorder=3)\n",
        "        ax.add_patch(hex_border_top_outline)\n",
        "        ax.add_patch(hex_border_bottom_outline)\n",
        "        ax.add_patch(hex_border_top)\n",
        "        ax.add_patch(hex_border_bottom)\n",
        "\n",
        "    for row in range(1, size-1):\n",
        "        x_left = row * horizontal_spacing/2\n",
        "        y = (size-(row+1))*vertical_spacing\n",
        "        x_right = row * horizontal_spacing/2 + (size-1) * horizontal_spacing\n",
        "        hex_border_left_outline = patches.RegularPolygon((x_left, y), numVertices=6, radius=hex_size, orientation=np.radians(0), edgecolor='black', facecolor='none', lw=20, zorder=2)\n",
        "        hex_border_right_outline = patches.RegularPolygon((x_right, y), numVertices=6, radius=hex_size, orientation=np.radians(0), edgecolor='black', facecolor='none', lw=20, zorder=2)\n",
        "        hex_border_left = patches.RegularPolygon((x_left, y), numVertices=6, radius=hex_size, orientation=np.radians(0), edgecolor=left_and_right_color, facecolor='none', lw=18, zorder=3)\n",
        "        hex_border_right = patches.RegularPolygon((x_right, y), numVertices=6, radius=hex_size, orientation=np.radians(0), edgecolor=left_and_right_color, facecolor='none', lw=18, zorder=3)\n",
        "        ax.add_patch(hex_border_left_outline)\n",
        "        ax.add_patch(hex_border_right_outline)\n",
        "        ax.add_patch(hex_border_left)\n",
        "        ax.add_patch(hex_border_right)\n",
        "\n",
        "    for row in range(size):\n",
        "        for col in range(size):\n",
        "            x = col * horizontal_spacing + row*horizontal_spacing/2\n",
        "            y = (size-(row+1)) * vertical_spacing\n",
        "\n",
        "            # Calculate centroid of the hexagon\n",
        "            #centroid_x, centroid_y = np.mean(hexagon.get_xy()[:-1], axis=0)\n",
        "            hexagon = patches.RegularPolygon((x, y), numVertices=6, radius=hex_size, orientation=np.radians(0), edgecolor='k', facecolor='none', lw=1, zorder=4)\n",
        "\n",
        "            # Add label at the centroid\n",
        "            #centroid_x, centroid_y = np.mean(vertices, axis=1)\n",
        "            centroid_x, centroid_y = hexagon.xy[0], hexagon.xy[1]\n",
        "            # label = f\"{''.join(str(x) for x in all_3d_node_coords[index_from_position(row, col)].to(int).tolist())}\"  # Adjust label format as needed\n",
        "            # ax.text(x, y, label, ha='center', va='center', color='green', fontsize=8, zorder=5)\n",
        "            # Calculate centroid of the hexagon\n",
        "\n",
        "            ax.add_patch(hexagon)\n",
        "            color_key = color_keys[row * size + col]\n",
        "            if probability_mode:\n",
        "                color = plt.cm.plasma(color_key)\n",
        "            else:\n",
        "                color = colors[np.argmax(color_key)]\n",
        "            hexagon.set_facecolor(color)\n",
        "\n",
        "    ax.autoscale_view()\n",
        "    plt.savefig('triplestate.pdf' if not probability_mode else 'tripleaction.pdf')\n",
        "    plt.show()\n",
        "\n",
        "if params['new_hex_grid']:\n",
        "    to_plot = torch.tile(torch.tensor([1,0,0]), (num_tiles, 1))\n",
        "    plot_hexagonal_grid_with_colors(board_width, states[80], rotated=False)\n",
        "    plot_hexagonal_grid_with_colors(board_width, actions[80], probability_mode=True)\n",
        "    #plot_hexagonal_grid_with_colors(board_width, to_plot, rotated=False)\n",
        "    print(outcomes[80])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwySE11B-24I"
      },
      "outputs": [],
      "source": [
        "#@title Build Datasets\n",
        "class MoHexGamesDataset(Dataset):\n",
        "    def __init__(self, states, actions, outcomes):\n",
        "        assert len(states) == len(actions) == len(outcomes)\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        self.outcomes = outcomes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        state = self.states[idx]\n",
        "        action = self.actions[idx]\n",
        "        outcome = self.outcomes[idx]\n",
        "        sample = (state, action, outcome)\n",
        "        return sample\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "if params['new_hex_grid'] or params['new_cnn'] or params['load_cnn'] or params['load_hex_grid']:\n",
        "    train_length = int(len(actions)*0.9)\n",
        "    dataset = MoHexGamesDataset(states, actions, outcomes)\n",
        "    training_data, test_data = random_split(dataset, [train_length, len(dataset)-train_length])\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "    training_dataloader = DataLoader(training_data, batch_size=params['batch_size'], shuffle=True, **kwargs)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=params['batch_size'], shuffle=True, **kwargs)\n",
        "\n",
        "if params['new_det_shannon'] or params['new_rni_shannon'] or params['new_sage'] or params['load_det_shannon'] or params['load_rni_shannon'] or params['load_sage']:\n",
        "    train_length = int(len(shannon_graphs)*0.9)\n",
        "    shannon_training_data, shannon_test_data = random_split(shannon_graphs, [train_length, len(shannon_graphs)-train_length])\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "    shannon_training_dataloader = ShannonDataLoader(shannon_training_data, batch_size=params['batch_size'], shuffle=True, **kwargs)\n",
        "    shannon_test_dataloader = ShannonDataLoader(shannon_test_data, batch_size=params['batch_size'], shuffle=True, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFnWMk8V5fHE"
      },
      "source": [
        "# Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFLdmP9hvmKT"
      },
      "outputs": [],
      "source": [
        "GNN_SCALING_FACTOR = 13.9123535\n",
        "\n",
        "class GNNLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.my_linear_map        = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "        self.neighbour_linear_map = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "        #self.my_linear_map.weight.data.normal_(mean=0., std=sqrt(1./(GNN_SCALING_FACTOR*hidden_dim)))\n",
        "        #self.neighbour_linear_map.weight.data.normal_(mean=0., std=sqrt(1./(GNN_SCALING_FACTOR*hidden_dim)))\n",
        "        self.output_values = []\n",
        "\n",
        "    def forward(self, node_embeddings, adj_matrix):\n",
        "        my_message = self.my_linear_map(node_embeddings)\n",
        "        neighbour_message = self.neighbour_linear_map(adj_matrix @ node_embeddings)\n",
        "        #neighbour_message = self.neighbour_linear_map(torch.sparse.mm(adj_matrix, node_embeddings))\n",
        "        return my_message + neighbour_message\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6_-n0o8VJY1"
      },
      "outputs": [],
      "source": [
        "#@title MLPs Within Layers\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        factor=2\n",
        "        self.first_layer = nn.Linear(hidden_dim, factor*hidden_dim)\n",
        "        self.batch_norm  = nn.BatchNorm1d(factor*hidden_dim)\n",
        "        self.last_layer  = nn.Linear(factor*hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first_layer(x)\n",
        "        #x = torch.transpose(x, -2, -1)\n",
        "        x = self.batch_norm(x)\n",
        "        #x = torch.transpose(x, -2, -1)\n",
        "        x = F.relu(x)\n",
        "        x = self.last_layer(x)\n",
        "        return x\n",
        "\n",
        "class MessageMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.first_layer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.batch_norm  = nn.BatchNorm1d(hidden_dim)\n",
        "        self.last_layer  = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first_layer(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.last_layer(x)\n",
        "        return x\n",
        "\n",
        "class TransposeMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.first_layer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.batch_norm  = nn.BatchNorm1d(hidden_dim)\n",
        "        self.last_layer  = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first_layer(x)\n",
        "        x = torch.transpose(x, -2, -1)\n",
        "        x = self.batch_norm(x)\n",
        "        x = torch.transpose(x, -2, -1)\n",
        "        x = F.relu(x)\n",
        "        x = self.last_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utMDkQMdShQL"
      },
      "outputs": [],
      "source": [
        "#@title Base Message Passing Layer\n",
        "class MPNNLayer(MessagePassing):\n",
        "    def __init__(self, input_dim, output_dim, transpose=False, aggr='add'):\n",
        "        super().__init__(aggr=aggr)\n",
        "        self.mlp_msg = TransposeMLP(input_dim, output_dim, output_dim) if transpose else MessageMLP(input_dim, output_dim, output_dim)\n",
        "        self.self_upd = TransposeMLP(input_dim, output_dim, output_dim) if transpose else MessageMLP(input_dim, output_dim, output_dim)\n",
        "\n",
        "    def forward(self, h, edge_index):\n",
        "        out = self.propagate(edge_index, h=h)\n",
        "        return out\n",
        "\n",
        "    def message(self, h_i, h_j):\n",
        "        return h_j\n",
        "\n",
        "    def aggregate(self, inputs, index):\n",
        "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
        "\n",
        "    def update(self, aggr_out, h):\n",
        "        return self.self_upd(h) + self.mlp_msg(aggr_out)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(emb_dim={hex_grid_params[\"hidden_dim\"]}, aggr={self.aggr})')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La9k2iaXC4B1"
      },
      "outputs": [],
      "source": [
        "#@title GIN Layer\n",
        "class GINLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = MLP(hidden_dim)\n",
        "        self.eps = nn.Parameter(torch.Tensor([0]), requires_grad=True)\n",
        "\n",
        "    def forward(self, node_embeddings, adj_matrix):\n",
        "        out = self.mlp((1 + self.eps) * node_embeddings + torch.matmul(adj_matrix, node_embeddings))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules\n"
      ],
      "metadata": {
        "id": "eFhqwjFwJ9MN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SGlpu9DDDua"
      },
      "outputs": [],
      "source": [
        "#@title Shannon Module\n",
        "class ShannonModule(nn.Module):\n",
        "    def __init__(self, layer_type, num_layers, input_dim, hidden_dim, drop_ratio, random_dims=0, use_batch_norm=True, use_jump_and_skip_connections=True):\n",
        "        super().__init__()\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_jump_and_skip_connections = use_jump_and_skip_connections\n",
        "        self.random_dims = random_dims\n",
        "        self.act_fun = F.relu\n",
        "\n",
        "        self.drop_ratio = drop_ratio\n",
        "        self.first_layer = nn.Linear(input_dim+random_dims, hidden_dim)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList() if self.use_batch_norm else None\n",
        "        # If we have any hidden layers, add them\n",
        "        for _ in range(num_layers):\n",
        "            self.hidden_layers.append(layer_type(hidden_dim, hidden_dim))\n",
        "            if self.use_batch_norm: self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        self.num_layers = len(self.hidden_layers)\n",
        "\n",
        "        self.policy_layer = MessageMLP(hidden_dim, hidden_dim, 1)\n",
        "        self.head_layer = nn.Sequential(\n",
        "            nn.Linear(3*hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        if self.random_dims > 0:\n",
        "            # Add random dims.\n",
        "            # x has shape [nodes, 2]\n",
        "            # we want to cat [nodes, random_dims]\n",
        "            # so, we want to cat along the 0th dimension\n",
        "            random_noise = torch.randn(x.shape[0], self.random_dims).to(device)\n",
        "            x = torch.cat([x, random_noise], dim=1)\n",
        "        x = self.first_layer(x)\n",
        "        h_list = []\n",
        "\n",
        "        for i, l in enumerate(self.hidden_layers):\n",
        "            prev_h = h_list[-1] if i > 0 else x\n",
        "            h = l(prev_h, edge_index)\n",
        "            if self.use_batch_norm: h = self.batch_norms[i](h)\n",
        "\n",
        "            if i != len(self.hidden_layers)-1:\n",
        "                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
        "            else:\n",
        "                h = F.dropout(h, self.drop_ratio, training = self.training)\n",
        "\n",
        "            if self.use_jump_and_skip_connections: h = prev_h + h\n",
        "            h_list.append(h)\n",
        "\n",
        "        node_representations = 0\n",
        "        if self.use_jump_and_skip_connections:\n",
        "            for h in h_list:\n",
        "                node_representations += h\n",
        "        else:\n",
        "            node_representations = h_list[-1]\n",
        "\n",
        "        pooled_sums_by_graph  = scatter(node_representations, batch, dim=0, reduce='sum')\n",
        "        pooled_means_by_graph = scatter(node_representations, batch, dim=0, reduce='sum')\n",
        "        pooled_maxs_by_graph  = scatter(node_representations, batch, dim=0, reduce='sum')\n",
        "        pooled_representations = torch.cat([pooled_sums_by_graph, pooled_maxs_by_graph, pooled_means_by_graph], dim=-1)\n",
        "\n",
        "        values = self.head_layer(pooled_representations)\n",
        "        values = torch.squeeze(torch.tanh(values))\n",
        "\n",
        "        policy = torch.squeeze(self.policy_layer(node_representations), dim=-1)\n",
        "        return (policy, values)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hex Grid Module\n",
        "class HexGridModule(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, drop_ratio, random_dims=0, use_batch_norm=True, use_jump_and_skip_connections=True):\n",
        "        super().__init__()\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_jump_and_skip_connections = use_jump_and_skip_connections\n",
        "        self.random_dims = random_dims\n",
        "        self.act_fun = F.relu\n",
        "\n",
        "        self.drop_ratio = drop_ratio\n",
        "        self.first_layer = nn.Linear(input_dim+random_dims, hidden_dim)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList() if self.use_batch_norm else None\n",
        "        # If we have any hidden layers, add them\n",
        "        for _ in range(num_layers):\n",
        "            self.hidden_layers.append(MPNNLayer(hidden_dim, hidden_dim, transpose=True))\n",
        "            if self.use_batch_norm: self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        self.num_layers = len(self.hidden_layers)\n",
        "\n",
        "        self.policy_layer = TransposeMLP(hidden_dim, hidden_dim, 1)\n",
        "        self.head_layer = nn.Sequential(\n",
        "            nn.Linear(3*hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        if self.random_dims > 0:\n",
        "            # Add random dims.\n",
        "            # x has shape [nodes, 2]\n",
        "            # we want to cat [nodes, random_dims]\n",
        "            # so, we want to cat along the 0th dimension\n",
        "            random_noise = torch.randn(x.shape[0], self.random_dims).to(device)\n",
        "            x = torch.cat([x, random_noise], dim=1)\n",
        "        x = self.first_layer(x)\n",
        "        h_list = []\n",
        "\n",
        "        for i, l in enumerate(self.hidden_layers):\n",
        "            prev_h = h_list[-1] if i > 0 else x\n",
        "\n",
        "            h = l(prev_h, edge_index)\n",
        "            if self.use_batch_norm:\n",
        "                h = torch.transpose(h, -2, -1)\n",
        "                h = self.batch_norms[i](h)\n",
        "                h = torch.transpose(h, -2, -1)\n",
        "\n",
        "            if i != len(self.hidden_layers)-1:\n",
        "                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
        "            else:\n",
        "                h = F.dropout(h, self.drop_ratio, training = self.training)\n",
        "\n",
        "            if self.use_jump_and_skip_connections: h = prev_h + h\n",
        "            h_list.append(h)\n",
        "\n",
        "        node_representations = 0\n",
        "        if self.use_jump_and_skip_connections:\n",
        "            for h in h_list:\n",
        "                node_representations += h\n",
        "        else:\n",
        "            node_representations = h_list[-1]\n",
        "\n",
        "        pooled_sum = torch.sum(node_representations, dim=1)\n",
        "        pooled_max, _ = torch.max(node_representations, dim=1)\n",
        "        pooled_mean = torch.mean(node_representations, dim=1)\n",
        "        pooled_representations = torch.cat([pooled_sum, pooled_max, pooled_mean], dim=-1)\n",
        "\n",
        "        values = self.head_layer(pooled_representations)\n",
        "        values = torch.squeeze(torch.tanh(values))\n",
        "        policy = torch.squeeze(self.policy_layer(node_representations), dim=-1)\n",
        "\n",
        "        return (policy, values)\n"
      ],
      "metadata": {
        "id": "5AaqvcF_jGb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x, adj_matrix, positions=None):\n",
        "        x = self.first_layer(x)\n",
        "        h_list = []\n",
        "\n",
        "        for i, l in enumerate(self.hidden_layers):\n",
        "            prev_h = h_list[-1] if i > 0 else x\n",
        "            if positions == None:\n",
        "                h = l(prev_h, adj_matrix)\n",
        "            else:\n",
        "                h = l(prev_h, positions, adj_matrix)\n",
        "\n",
        "            h = torch.transpose(h, -2, -1)\n",
        "            h = self.batch_norms[i](h)\n",
        "            h = torch.transpose(h, -2, -1)\n",
        "\n",
        "            if i != len(self.hidden_layers)-1:\n",
        "                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
        "            else:\n",
        "                h = F.dropout(h, self.drop_ratio, training = self.training)\n",
        "\n",
        "            h = prev_h + h\n",
        "            h_list.append(h)\n",
        "\n",
        "        node_representations = 0\n",
        "        for h in h_list:\n",
        "            node_representations += h\n",
        "\n",
        "        pooled_sum = torch.sum(node_representations, dim=1)\n",
        "        pooled_max, _ = torch.max(node_representations, dim=1)\n",
        "        pooled_mean = torch.mean(node_representations, dim=1)\n",
        "        pooled_representation = torch.cat([pooled_sum, pooled_max, pooled_mean], dim=-1)\n",
        "\n",
        "        policy = torch.squeeze(self.policy_layer(node_representations), dim=-1)\n",
        "\n",
        "        value = torch.tanh(self.head_mlp(pooled_representation))\n",
        "\n",
        "        return torch.cat((policy, value), dim=-1)\n"
      ],
      "metadata": {
        "id": "5lvzLXSkKcVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9yppOsspqps"
      },
      "outputs": [],
      "source": [
        "#@title CNN Module\n",
        "# Blue too low means factor too high, blue too high means factor too low\n",
        "CNN_SCALING_FACTOR = 4.125\n",
        "CNN_SCALING_FACTOR = 4\n",
        "\n",
        "class CNNModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = 'CNN'\n",
        "        self.act_fun = F.relu\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.drop_ratio = cnn_params['drop_ratio']\n",
        "\n",
        "        self.layers.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels=cnn_params['input_dim'],\n",
        "                out_channels=cnn_params['hidden_dim'],\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            )\n",
        "        )\n",
        "\n",
        "        for _ in range(cnn_params['num_layers']-2):\n",
        "            self.layers.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=cnn_params['hidden_dim'],\n",
        "                    out_channels=cnn_params['hidden_dim'],\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=1\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.layers.append(\n",
        "            nn.Conv2d(\n",
        "                in_channels=cnn_params['hidden_dim'],\n",
        "                out_channels=1,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.num_layers = len(self.layers)\n",
        "\n",
        "        self.policy_layer = TransposeMLP(cnn_params['hidden_dim'], cnn_params['hidden_dim'], 1)\n",
        "        self.head_layer = nn.Sequential(\n",
        "            nn.Linear(3*cnn_params['hidden_dim'], cnn_params['hidden_dim']),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(cnn_params['hidden_dim'], 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        hidden_layers = self.layers[:-1]\n",
        "        last_layer = self.layers[-1]\n",
        "\n",
        "        for l in hidden_layers:\n",
        "            x = l(x)\n",
        "            x = self.act_fun(x)\n",
        "            x = F.dropout(x, self.drop_ratio, training = self.training)\n",
        "\n",
        "        #x = last_layer(x)\n",
        "        x = torch.reshape(x, (-1,num_tiles,cnn_params['hidden_dim']))\n",
        "\n",
        "        pooled_sum = torch.sum(x, dim=1)\n",
        "        pooled_max, _ = torch.max(x, dim=1)\n",
        "        pooled_mean = torch.mean(x, dim=1)\n",
        "        pooled_representation = torch.cat([pooled_sum, pooled_max, pooled_mean], dim=-1)\n",
        "\n",
        "        values = self.head_layer(pooled_representation)\n",
        "        values = torch.squeeze(torch.tanh(values))\n",
        "        policy = torch.squeeze(self.policy_layer(x), dim=-1)\n",
        "\n",
        "        return (policy, values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFgDuHBc36DB"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zie7rvtNGVi7"
      },
      "outputs": [],
      "source": [
        "#@title Model Wrapper\n",
        "class Model:\n",
        "    def __init__(self, name, module, optimiser):\n",
        "        self.name = name\n",
        "        self.module = module\n",
        "        self.optimiser = optimiser\n",
        "        self.epochs_trained = 0\n",
        "        self.performance_degradation_streak = 0\n",
        "        self.early_stop_threshold = params['early_stop_threshold']\n",
        "\n",
        "        self.test_losses = {'action':[], 'outcome':[], 'combined':[]}\n",
        "        self.test_accuracies = {'action':[], 'outcome':[]}\n",
        "\n",
        "        self.training_losses = {'action':[], 'outcome':[], 'combined':[]}\n",
        "        self.training_accuracies = {'action':[], 'outcome':[]}\n",
        "\n",
        "        self.lowest_combined_loss = 100000\n",
        "\n",
        "        self.continue_training = True if self.performance_degradation_streak < self.early_stop_threshold else False\n",
        "\n",
        "    def update(self, new_test_losses, new_test_accuracies, new_training_losses, new_training_accuracies):\n",
        "        if self.continue_training:\n",
        "            self.epochs_trained += 1\n",
        "\n",
        "            self.test_losses['action'].append(new_test_losses['action'])\n",
        "            self.test_losses['outcome'].append(new_test_losses['outcome'])\n",
        "            self.test_losses['combined'].append(new_test_losses['combined'])\n",
        "            self.test_accuracies['action'].append(new_test_accuracies['action'])\n",
        "            self.test_accuracies['outcome'].append(new_test_accuracies['outcome'])\n",
        "\n",
        "            self.training_losses['action'].append(new_training_losses['action'])\n",
        "            self.training_losses['outcome'].append(new_training_losses['outcome'])\n",
        "            self.training_losses['combined'].append(new_training_losses['combined'])\n",
        "            self.training_accuracies['action'].append(new_training_accuracies['action'])\n",
        "            self.training_accuracies['outcome'].append(new_training_accuracies['outcome'])\n",
        "\n",
        "        if test_losses['combined'] >= self.lowest_combined_loss:\n",
        "            self.performance_degradation_streak += 1\n",
        "        elif test_losses['combined'] < self.lowest_combined_loss:\n",
        "            self.lowest_combined_loss = test_losses['combined']\n",
        "            self.performance_degradation_streak = 0\n",
        "\n",
        "        if self.performance_degradation_streak >= self.early_stop_threshold:\n",
        "            self.continue_training = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8UxClkVrkgD"
      },
      "outputs": [],
      "source": [
        "#@title Build and Load Models\n",
        "all_models = []\n",
        "\n",
        "if params['new_hex_grid']:\n",
        "    mpnn_module = HexGridModule(num_layers=hex_grid_params['num_layers'],\n",
        "                                input_dim=hex_grid_params['input_dim'],\n",
        "                                hidden_dim=hex_grid_params['hidden_dim'],\n",
        "                                drop_ratio=hex_grid_params['drop_ratio'],\n",
        "                                use_batch_norm=True,\n",
        "                                use_jump_and_skip_connections=False).to(device)\n",
        "    mpnn_optimiser = torch.optim.Adam(mpnn_module.parameters(), lr=hex_grid_params['learning_rate'], weight_decay=hex_grid_params['weight_decay'])\n",
        "    all_models.append(Model('HexGridGNN', mpnn_module, mpnn_optimiser))\n",
        "\n",
        "if params['new_det_shannon']:\n",
        "    det_shannon_module = ShannonModule(layer_type=MPNNLayer,\n",
        "                                       num_layers=det_shannon_params['num_layers'],\n",
        "                                       input_dim=det_shannon_params['input_dim'],\n",
        "                                       hidden_dim=det_shannon_params['hidden_dim'],\n",
        "                                       drop_ratio=det_shannon_params['drop_ratio'],\n",
        "                                       use_batch_norm=True,\n",
        "                                       use_jump_and_skip_connections=False).to(device)\n",
        "    det_shannon_optimiser = torch.optim.Adam(det_shannon_module.parameters(), lr=det_shannon_params['learning_rate'], weight_decay=det_shannon_params['weight_decay'])\n",
        "    all_models.append(Model('ShannonGNN', det_shannon_module, det_shannon_optimiser))\n",
        "\n",
        "if params['new_rni_shannon']:\n",
        "    rni_shannon_module = ShannonModule(layer_type=MPNNLayer,\n",
        "                                       num_layers=rni_shannon_params['num_layers'],\n",
        "                                       input_dim=rni_shannon_params['input_dim'],\n",
        "                                       hidden_dim=rni_shannon_params['hidden_dim'],\n",
        "                                       drop_ratio=rni_shannon_params['drop_ratio'],\n",
        "                                       random_dims=rni_shannon_params['random_dims'],\n",
        "                                       use_batch_norm=True,\n",
        "                                       use_jump_and_skip_connections=False).to(device)\n",
        "\n",
        "    rni_shannon_optimiser = torch.optim.Adam(rni_shannon_module.parameters(),\n",
        "                                             lr=rni_shannon_params['learning_rate'],\n",
        "                                             weight_decay=rni_shannon_params['weight_decay'])\n",
        "\n",
        "    all_models.append(Model('ShannonGNN-RNI', rni_shannon_module, rni_shannon_optimiser))\n",
        "\n",
        "if params['new_cnn']:\n",
        "    cnn_module = CNNModule().to(device)\n",
        "    cnn_optimiser = torch.optim.Adam(cnn_module.parameters(), lr=cnn_params['learning_rate'], weight_decay=cnn_params['weight_decay'])\n",
        "    all_models.append(Model('CNN', cnn_module, cnn_optimiser))\n",
        "\n",
        "if params['new_sage']:\n",
        "    sage_module = ShannonModule(layer_type = SAGEConv,\n",
        "                            num_layers = sage_params['num_layers'],\n",
        "                            input_dim = sage_params['input_dim'],\n",
        "                            hidden_dim = sage_params['hidden_dim'],\n",
        "                            drop_ratio = sage_params['drop_ratio'],\n",
        "                            use_batch_norm=False,\n",
        "                            use_jump_and_skip_connections=False,\n",
        "                            ).to(device)\n",
        "    sage_optimiser = torch.optim.Adam(sage_module.parameters(), lr=sage_params['learning_rate'], weight_decay=sage_params['weight_decay'])\n",
        "    all_models.append(Model('GraphSAGE', sage_module, sage_optimiser))\n",
        "\n",
        "all_models = []\n",
        "for filename in model_checkpoint_filenames_to_load:\n",
        "    model = torch.load(filename)\n",
        "    all_models.append(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elufj5kegEoS"
      },
      "outputs": [],
      "source": [
        "#@title Plot Initial Layer Outputs\n",
        "def get_weights_by_layer(model):\n",
        "    weights_by_layer = []\n",
        "    for layer in model.layers:\n",
        "        weights_this_layer = []\n",
        "        for child in layer.modules():\n",
        "            if hasattr(child, 'weight'):\n",
        "                weights_this_layer.extend(\n",
        "                    torch.reshape(child.weight.data, (-1,)).detach().cpu().numpy())\n",
        "        weights_by_layer.append(weights_this_layer)\n",
        "    return weights_by_layer\n",
        "\n",
        "def plot_weights_by_layer_histogram(weights_by_layer, title, bins=[10 for _ in range(100)]):\n",
        "    for i, weights in enumerate(weights_by_layer):\n",
        "        plt.hist(weights, bins=bins[i], label=i+1, histtype=u'step', density=True)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def get_outputs_by_layer(model):\n",
        "    layers = model.module.layers\n",
        "    outputs_by_layer = [[] for _ in range(len(layers))]\n",
        "    for data in training_dataloader:\n",
        "        x = data[0].to(device)\n",
        "        if model.name == 'CNN':\n",
        "            x = torch.reshape(x, (-1, 3, board_width, board_width))\n",
        "        for i, layer in enumerate(layers):\n",
        "            # Pass in the adjacency matrix if we are using a GNN, otherwise don't.\n",
        "            # Calculate the outputs for this batch.\n",
        "            x = layer(x, adj_matrix) if model.name == 'GNN' else layer(x)\n",
        "            # Combine these batch outputs into a big, flat array of scalars.\n",
        "            output_values_array = torch.reshape(x, (-1,)).detach().cpu().numpy()\n",
        "            # Calculate how many outputs values to sample from this big array.\n",
        "            num_to_sample = max(len(x)//len(layers), 1)\n",
        "            # Unpack this sample into the array of samples for this layer over all batches.\n",
        "            outputs_by_layer[i].extend(np.random.choice(output_values_array, num_to_sample))\n",
        "            # Apply the activation funcion, ready for the next layer.\n",
        "            x = model.module.act_fun(x)\n",
        "\n",
        "    return outputs_by_layer\n",
        "\n",
        "if params['plot_initial_outputs']:\n",
        "    for model in all_models:\n",
        "        initial_outputs_by_layer = get_outputs_by_layer(model)\n",
        "        plot_weights_by_layer_histogram(initial_outputs_by_layer, title=f'{model.name} Initial Outputs by Layer')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV8cusSCf044"
      },
      "outputs": [],
      "source": [
        "#@title Plot all Models\n",
        "def plot_results(title, xlabel, ylabel, metric, loss_mode = True):\n",
        "    max_e = 0\n",
        "    for i, model in enumerate(all_models):\n",
        "        training_values = model.training_losses[metric] if loss_mode else model.training_accuracies[metric]\n",
        "        test_values = model.test_losses[metric] if loss_mode else model.test_accuracies[metric]\n",
        "        e = max(len(training_values), len(test_values))\n",
        "        if e > max_e:\n",
        "            max_e = e\n",
        "        training_xs = range(1,len(training_values)+1)\n",
        "        test_xs = range(1,len(test_values)+1)\n",
        "        training_loss_line = plt.plot(training_xs, training_values, label=f'{model.name} training')[0]\n",
        "        plt.plot(test_xs, test_values, '--', label=f'{model.name} test', color=training_loss_line.get_color())\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "    filename = f'drive/MyDrive/Colab Notebooks/hexbot/diagrams/E{max_e}-accuracies-{random.randint(1,1000)}.pdf' if not loss_mode else f'drive/MyDrive/Colab Notebooks/hexbot/diagrams/E{max_e}-losses-{random.randint(1,1000)}.pdf'\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "def display_all_model_results():\n",
        "    plot_results('Training and Test Policy Loss by Epoch', 'Epoch', 'Loss', 'action', True)\n",
        "    plot_results('Training and Test Value Loss by Epoch', 'Epoch', 'Loss', 'outcome', True)\n",
        "    plot_results('Training and Test Combined Loss by Epoch', 'Epoch', 'Loss', 'combined', True)\n",
        "\n",
        "    plot_results('Training and Test Policy Accuracy by Epoch', 'Epoch', 'Accuracy', 'action', False)\n",
        "    plot_results('Training and Test Value Accuracy by Epoch', 'Epoch', 'Accuracy', 'outcome', False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFN-cfksrBci"
      },
      "outputs": [],
      "source": [
        "#@title Model Saver\n",
        "def save_model(model):\n",
        "    filename = f'drive/MyDrive/Colab Notebooks/hexbot/temp/action-{round(max(model.test_accuracies[\"action\"]), 2)}_E{model.epochs_trained}_{model.name}_D{model.module.num_layers}_outcome{round(max(model.test_accuracies[\"outcome\"]), 2)}.pt'\n",
        "    torch.save(model, filename)\n",
        "\n",
        "    if model.name == 'GNN' or model.name == 'GIN':\n",
        "        example = torch.rand(1, num_tiles, gnn_params['input_dim']).to(device)\n",
        "        traced_script_module = torch.jit.trace(model.module, example_kwarg_inputs={'edge_index':adj_matrix, 'x':example})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqEIOry7trsX"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRGC6gUeZul-"
      },
      "outputs": [],
      "source": [
        "#@title Get Model Outputs on Data\n",
        "def get_model_outputs_on_data(model, x, true_actions, true_outcomes):\n",
        "    if model.name == 'CNN':\n",
        "        x = torch.reshape(x, (-1, cnn_params['input_dim'], board_width, board_width))\n",
        "        policy_head_output, value_head_output = model.module(x)\n",
        "    elif model.name == 'HexGridGNN':\n",
        "        policy_head_output, value_head_output = model.module(x, global_edge_index)\n",
        "\n",
        "    losses = {}\n",
        "    losses['action']   = params['policy_loss_fn'](policy_head_output, true_actions)\n",
        "    losses['outcome']  = params['value_loss_fn'](value_head_output, true_outcomes.float())\n",
        "    losses['combined'] = torch.add(params['loss_weight'] * losses['action'], (1-params['loss_weight']) * losses['outcome'])\n",
        "\n",
        "    action_votes  = torch.argmax(F.softmax(policy_head_output, dim=0), dim=1)\n",
        "    outcome_votes = torch.sign(value_head_output)\n",
        "\n",
        "    true_actions = torch.argmax(true_actions, dim=1)\n",
        "\n",
        "    actions_predicted_correctly = torch.sum(true_actions == action_votes).item()\n",
        "    outcomes_predicted_correctly = torch.sum(true_outcomes == outcome_votes).item()\n",
        "    accuracies = {\n",
        "        'action':actions_predicted_correctly/len(x),\n",
        "        'outcome':outcomes_predicted_correctly/len(x)\n",
        "    }\n",
        "\n",
        "    return losses, accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get Shannon Model Outputs on Data\n",
        "def sum_pool(x, node_to_graph_map):\n",
        "   # Implement the function here\n",
        "   return scatter(x, node_to_graph_map, dim=0, reduce=\"sum\")\n",
        "\n",
        "def get_shannon_model_outputs_on_data(model, x, edge_index, true_actions, true_outcomes, first_index_of_each_batch=None, batch=None):\n",
        "    policy_head_output, value_head_output = model.module(x, edge_index, batch)\n",
        "\n",
        "    policy_by_graph = torch.tensor_split(policy_head_output, first_index_of_each_batch[1:-1])\n",
        "    losses = {'action': 0.0}\n",
        "    action_votes = torch.zeros(len(true_actions)).to(device)\n",
        "    for i, policy in enumerate(policy_by_graph):\n",
        "        losses['action'] += params['policy_loss_fn'](policy, true_actions[i])\n",
        "        action_votes[i] = torch.argmax(F.softmax(policy, dim=0), dim=-1)\n",
        "\n",
        "    losses['action'] /= len(policy_by_graph)\n",
        "\n",
        "    losses['outcome']  = params['value_loss_fn'](value_head_output, true_outcomes.float())\n",
        "    losses['combined'] = torch.add(params['loss_weight'] * losses['action'], (1-params['loss_weight']) * losses['outcome'])\n",
        "\n",
        "    outcome_votes = torch.sign(value_head_output)\n",
        "\n",
        "    actions_predicted_correctly = torch.sum(true_actions == action_votes).item()\n",
        "    outcomes_predicted_correctly = torch.sum(true_outcomes == outcome_votes).item()\n",
        "    accuracies = {\n",
        "        'action':actions_predicted_correctly/len(true_actions),\n",
        "        'outcome':outcomes_predicted_correctly/len(true_actions)\n",
        "    }\n",
        "\n",
        "    return losses, accuracies"
      ],
      "metadata": {
        "id": "qUKAXZNiMnSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8nfknLS4MVe"
      },
      "outputs": [],
      "source": [
        "#@title Model Training Iteration\n",
        "def train_one_epoch(model):\n",
        "    total_chunk_losses = {'action':0.0, 'outcome':0.0, 'combined':0.0}\n",
        "    sum_of_chunk_accuracies = {'action':0.0, 'outcome':0.0}\n",
        "\n",
        "    if model.name == 'HexGridGNN' or model.name == 'CNN':\n",
        "        num_batches_in_chunk = ceil(len(actions)*0.9/params['batch_size'] * 0.1)\n",
        "        loader = training_dataloader\n",
        "    else:\n",
        "        num_batches_in_chunk = ceil(len(shannon_graphs)*0.9/params['batch_size'] * 0.1)\n",
        "        loader = shannon_training_dataloader\n",
        "\n",
        "    model.module.train()\n",
        "    for i, data in enumerate(loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        if model.name == 'HexGridGNN' or model.name == 'CNN':\n",
        "            x, true_actions, game_outcomes = data[0].to(device), data[1].to(device), data[2].to(device)\n",
        "            batch_losses, batch_accuracies = get_model_outputs_on_data(model, x, true_actions, game_outcomes)\n",
        "        else:\n",
        "            x, edge_index, true_actions, game_outcomes, first_index_of_each_batch, batch = data.x.to(device), data.edge_index.to(device), data.y.to(device), data.outcome.to(device), data.ptr, data.batch.to(device)\n",
        "            batch_losses, batch_accuracies = get_shannon_model_outputs_on_data(model, x, edge_index, true_actions, game_outcomes, first_index_of_each_batch, batch)\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        model.optimiser.zero_grad()\n",
        "        # Compute the loss gradients\n",
        "        batch_losses['combined'].backward()\n",
        "        # Adjust learning weights along this gradient\n",
        "        model.optimiser.step()\n",
        "        # Gather data and report\n",
        "        total_chunk_losses['action'] += batch_losses['action'].item()\n",
        "        total_chunk_losses['outcome'] += batch_losses['outcome'].item()\n",
        "        total_chunk_losses['combined'] += batch_losses['combined'].item()\n",
        "\n",
        "        sum_of_chunk_accuracies['action'] += batch_accuracies['action']\n",
        "        sum_of_chunk_accuracies['outcome'] += batch_accuracies['outcome']\n",
        "\n",
        "        if i % num_batches_in_chunk == num_batches_in_chunk-1:\n",
        "            avg_losses_this_chunk = {\n",
        "                'action':total_chunk_losses['action'] / num_batches_in_chunk,\n",
        "                'outcome':total_chunk_losses['outcome'] / num_batches_in_chunk,\n",
        "                'combined':total_chunk_losses['combined'] / num_batches_in_chunk\n",
        "            }\n",
        "            avg_accuracies_this_chunk = {\n",
        "                'action':sum_of_chunk_accuracies['action'] / num_batches_in_chunk,\n",
        "                'outcome':sum_of_chunk_accuracies['outcome'] / num_batches_in_chunk,\n",
        "            }\n",
        "\n",
        "            print(f'    batch {(i+1)//num_batches_in_chunk} loss: {tuple(avg_losses_this_chunk.values())} acc: {tuple(avg_accuracies_this_chunk.values())}')\n",
        "            total_chunk_losses = {'action':0.0, 'outcome':0.0, 'combined':0.0}\n",
        "            sum_of_chunk_accuracies = {'action':0.0, 'outcome':0.0}\n",
        "\n",
        "    return avg_losses_this_chunk, avg_accuracies_this_chunk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drl3IqE36eoR"
      },
      "outputs": [],
      "source": [
        "#@title Model Testing Iteration\n",
        "def test_model(model):\n",
        "    total_losses = {'action':0, 'outcome':0, 'combined':0}\n",
        "    sum_of_accuracies = {'action':0, 'outcome':0}\n",
        "\n",
        "    if model.name == 'GNN' or model.name == 'GIN' or model.name == 'HexGridGNN' or model.name == 'CNN':\n",
        "        loader = test_dataloader\n",
        "    else:\n",
        "        loader = shannon_test_dataloader\n",
        "\n",
        "    model.module.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            if model.name == 'HexGridGNN' or model.name == 'CNN':\n",
        "                x, label_distributions, game_outcomes = data[0].to(device), data[1].to(device), data[2].to(device)\n",
        "                losses, accuracies = get_model_outputs_on_data(model, x, label_distributions, game_outcomes)\n",
        "            else:\n",
        "                x, edge_index, label_distributions, game_outcomes, first_index_of_each_batch, batch = data.x.to(device), data.edge_index.to(device), data.y.to(device), data.outcome.to(device), data.ptr, data.batch.to(device)\n",
        "                losses, accuracies = get_shannon_model_outputs_on_data(model, x, edge_index, label_distributions, game_outcomes, first_index_of_each_batch, batch)\n",
        "\n",
        "            total_losses['action'] += losses['action'].item()\n",
        "            total_losses['outcome'] += losses['outcome'].item()\n",
        "            total_losses['combined'] += losses['combined'].item()\n",
        "\n",
        "            sum_of_accuracies['action'] += accuracies['action']\n",
        "            sum_of_accuracies['outcome'] += accuracies['outcome']\n",
        "\n",
        "    num_examples = len(loader)\n",
        "\n",
        "    avg_losses = {\n",
        "        'action'  : total_losses['action'] / num_examples,\n",
        "        'outcome' : total_losses['outcome'] / num_examples,\n",
        "        'combined': total_losses['combined'] / num_examples\n",
        "    }\n",
        "    avg_accuracies = {\n",
        "        'action'  : sum_of_accuracies['action'] / num_examples,\n",
        "        'outcome' : sum_of_accuracies['outcome'] / num_examples\n",
        "    }\n",
        "\n",
        "    return avg_losses, avg_accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPT92T7D7DYJ"
      },
      "outputs": [],
      "source": [
        "#@title Training Loop\n",
        "\n",
        "for epoch in range(params['num_epochs']):\n",
        "    print('EPOCH {}:'.format(epoch + 1))\n",
        "    for model in all_models:\n",
        "        if model.continue_training:\n",
        "            print(f'  {model.name}:')\n",
        "            training_losses, training_accuracies = train_one_epoch(model)\n",
        "            test_losses, test_accuracies = test_model(model)\n",
        "\n",
        "            model.update(test_losses, test_accuracies,\n",
        "                         training_losses, training_accuracies)\n",
        "\n",
        "            #if params['save_interval'] > 0 and test_accuracies['action'] > best_test_accuracy:\n",
        "            #    best_test_accuracy = test_accuracies['action']\n",
        "            #    save_model(model)\n",
        "\n",
        "            print(f'    {model.name} TEST LOSS {tuple(test_losses.values())} TEST ACC {tuple(test_accuracies.values())}')\n",
        "        else:\n",
        "            print(f'{model.name} Skipped')\n",
        "\n",
        "    if params['plot_interval'] > 0:\n",
        "        if epoch % params['plot_interval'] == params['plot_interval']-1:\n",
        "            display_all_model_results()\n",
        "    if params['save_interval'] > 0:\n",
        "        if epoch % params['save_interval'] == params['save_interval']-1:\n",
        "            for model in all_models:\n",
        "                save_model(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BEqJ7Lkmjyj"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGI6Fo7n3VBz"
      },
      "outputs": [],
      "source": [
        "#@title Final Results Display\n",
        "if params['save_interval'] > 0:\n",
        "    for model in all_models:\n",
        "        save_model(model)\n",
        "\n",
        "display_all_model_results()\n",
        "\n",
        "for i, model in enumerate(all_models):\n",
        "    print(f'{model.name} - Best test action accuracy:', max(model.test_accuracies['action']))\n",
        "    print(f'{model.name} - Best test outcome accuracy:', max(model.test_accuracies['outcome']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN5yMw20gzUT"
      },
      "outputs": [],
      "source": [
        "#@title Output Parameter Count\n",
        "from itertools import chain\n",
        "\n",
        "for model in all_models:\n",
        "    total_params = sum(p.numel() for p in model.module.parameters())\n",
        "    print(model.name, total_params)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
